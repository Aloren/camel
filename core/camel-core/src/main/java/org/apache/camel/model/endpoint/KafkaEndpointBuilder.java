/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.camel.model.endpoint;

import java.util.concurrent.ExecutorService;
import javax.annotation.Generated;
import org.apache.camel.ExchangePattern;
import org.apache.camel.model.AbstractEndpointBuilder;
import org.apache.camel.model.EndpointConsumerBuilder;
import org.apache.camel.model.EndpointProducerBuilder;
import org.apache.camel.spi.ExceptionHandler;
import org.apache.camel.spi.HeaderFilterStrategy;
import org.apache.camel.spi.StateRepository;

/**
 * The kafka component allows messages to be sent to (or consumed from) Apache
 * Kafka brokers.
 * 
 * Generated by camel-package-maven-plugin - do not edit this file!
 */
@Generated("org.apache.camel.maven.packaging.EndpointDslMojo")
public interface KafkaEndpointBuilder {


    public static class KafkaCommonBuilder<T extends AbstractEndpointBuilder>
            extends
                AbstractEndpointBuilder<T> {
        KafkaCommonBuilder(String path) {
            super("kafka", path);
        }
        /**
         * Name of the topic to use. On the consumer you can use comma to
         * separate multiple topics. A producer can only send a message to a
         * single topic. The option is a java.lang.String type.
         */
        public T topic(String topic) {
            this.properties.put("topic", topic);
            return (T) this;
        }
        /**
         * URL of the Kafka brokers to use. The format is
         * host1:port1,host2:port2, and the list can be a subset of brokers or a
         * VIP pointing to a subset of brokers. This option is known as
         * bootstrap.servers in the Kafka documentation. The option is a
         * java.lang.String type.
         */
        public T brokers(String brokers) {
            this.properties.put("brokers", brokers);
            return (T) this;
        }
        /**
         * The client id is a user-specified string sent in each request to help
         * trace calls. It should logically identify the application making the
         * request. The option is a java.lang.String type.
         */
        public T clientId(String clientId) {
            this.properties.put("clientId", clientId);
            return (T) this;
        }
        /**
         * To use a custom HeaderFilterStrategy to filter header to and from
         * Camel message. The option is a
         * org.apache.camel.spi.HeaderFilterStrategy type.
         */
        public T headerFilterStrategy(HeaderFilterStrategy headerFilterStrategy) {
            this.properties.put("headerFilterStrategy", headerFilterStrategy);
            return (T) this;
        }
        /**
         * The maximum amount of time in milliseconds to wait when reconnecting
         * to a broker that has repeatedly failed to connect. If provided, the
         * backoff per host will increase exponentially for each consecutive
         * connection failure, up to this maximum. After calculating the backoff
         * increase, 20% random jitter is added to avoid connection storms. The
         * option is a java.lang.Integer type.
         */
        public T reconnectBackoffMaxMs(Integer reconnectBackoffMaxMs) {
            this.properties.put("reconnectBackoffMaxMs", reconnectBackoffMaxMs);
            return (T) this;
        }
        /**
         * Whether the endpoint should use basic property binding (Camel 2.x) or
         * the newer property binding with additional capabilities. The option
         * is a boolean type.
         */
        public T basicPropertyBinding(boolean basicPropertyBinding) {
            this.properties.put("basicPropertyBinding", basicPropertyBinding);
            return (T) this;
        }
        /**
         * Sets whether synchronous processing should be strictly used, or Camel
         * is allowed to use asynchronous processing (if supported). The option
         * is a boolean type.
         */
        public T synchronous(boolean synchronous) {
            this.properties.put("synchronous", synchronous);
            return (T) this;
        }
        /**
         * Sets interceptors for producer or consumers. Producer interceptors
         * have to be classes implementing
         * org.apache.kafka.clients.producer.ProducerInterceptor Consumer
         * interceptors have to be classes implementing
         * org.apache.kafka.clients.consumer.ConsumerInterceptor Note that if
         * you use Producer interceptor on a consumer it will throw a class cast
         * exception in runtime. The option is a java.lang.String type.
         */
        public T interceptorClasses(String interceptorClasses) {
            this.properties.put("interceptorClasses", interceptorClasses);
            return (T) this;
        }
        /**
         * Login thread sleep time between refresh attempts. The option is a
         * java.lang.Integer type.
         */
        public T kerberosBeforeReloginMinTime(
                Integer kerberosBeforeReloginMinTime) {
            this.properties.put("kerberosBeforeReloginMinTime", kerberosBeforeReloginMinTime);
            return (T) this;
        }
        /**
         * Kerberos kinit command path. Default is /usr/bin/kinit. The option is
         * a java.lang.String type.
         */
        public T kerberosInitCmd(String kerberosInitCmd) {
            this.properties.put("kerberosInitCmd", kerberosInitCmd);
            return (T) this;
        }
        /**
         * A list of rules for mapping from principal names to short names
         * (typically operating system usernames). The rules are evaluated in
         * order and the first rule that matches a principal name is used to map
         * it to a short name. Any later rules in the list are ignored. By
         * default, principal names of the form {username}/{hostname}{REALM} are
         * mapped to {username}. For more details on the format please see the
         * security authorization and acls documentation.. Multiple values can
         * be separated by comma. The option is a java.lang.String type.
         */
        public T kerberosPrincipalToLocalRules(
                String kerberosPrincipalToLocalRules) {
            this.properties.put("kerberosPrincipalToLocalRules", kerberosPrincipalToLocalRules);
            return (T) this;
        }
        /**
         * Percentage of random jitter added to the renewal time. The option is
         * a java.lang.Double type.
         */
        public T kerberosRenewJitter(Double kerberosRenewJitter) {
            this.properties.put("kerberosRenewJitter", kerberosRenewJitter);
            return (T) this;
        }
        /**
         * Login thread will sleep until the specified window factor of time
         * from last refresh to ticket's expiry has been reached, at which time
         * it will try to renew the ticket. The option is a java.lang.Double
         * type.
         */
        public T kerberosRenewWindowFactor(Double kerberosRenewWindowFactor) {
            this.properties.put("kerberosRenewWindowFactor", kerberosRenewWindowFactor);
            return (T) this;
        }
        /**
         * Expose the kafka sasl.jaas.config parameter Example:
         * org.apache.kafka.common.security.plain.PlainLoginModule required
         * username=USERNAME password=PASSWORD;. The option is a
         * java.lang.String type.
         */
        public T saslJaasConfig(String saslJaasConfig) {
            this.properties.put("saslJaasConfig", saslJaasConfig);
            return (T) this;
        }
        /**
         * The Kerberos principal name that Kafka runs as. This can be defined
         * either in Kafka's JAAS config or in Kafka's config. The option is a
         * java.lang.String type.
         */
        public T saslKerberosServiceName(String saslKerberosServiceName) {
            this.properties.put("saslKerberosServiceName", saslKerberosServiceName);
            return (T) this;
        }
        /**
         * The Simple Authentication and Security Layer (SASL) Mechanism used.
         * For the valid values see a href=
         * http://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtmlhttp://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtml. The option is a java.lang.String type.
         */
        public T saslMechanism(String saslMechanism) {
            this.properties.put("saslMechanism", saslMechanism);
            return (T) this;
        }
        /**
         * Protocol used to communicate with brokers. SASL_PLAINTEXT, PLAINTEXT
         * and SSL are supported. The option is a java.lang.String type.
         */
        public T securityProtocol(String securityProtocol) {
            this.properties.put("securityProtocol", securityProtocol);
            return (T) this;
        }
        /**
         * A list of cipher suites. This is a named combination of
         * authentication, encryption, MAC and key exchange algorithm used to
         * negotiate the security settings for a network connection using TLS or
         * SSL network protocol.By default all the available cipher suites are
         * supported. The option is a java.lang.String type.
         */
        public T sslCipherSuites(String sslCipherSuites) {
            this.properties.put("sslCipherSuites", sslCipherSuites);
            return (T) this;
        }
        /**
         * SSL configuration using a Camel SSLContextParameters object. If
         * configured it's applied before the other SSL endpoint parameters. The
         * option is a org.apache.camel.support.jsse.SSLContextParameters type.
         */
        public T sslContextParameters(Object sslContextParameters) {
            this.properties.put("sslContextParameters", sslContextParameters);
            return (T) this;
        }
        /**
         * The list of protocols enabled for SSL connections. TLSv1.2, TLSv1.1
         * and TLSv1 are enabled by default. The option is a java.lang.String
         * type.
         */
        public T sslEnabledProtocols(String sslEnabledProtocols) {
            this.properties.put("sslEnabledProtocols", sslEnabledProtocols);
            return (T) this;
        }
        /**
         * The endpoint identification algorithm to validate server hostname
         * using server certificate. The option is a java.lang.String type.
         */
        public T sslEndpointAlgorithm(String sslEndpointAlgorithm) {
            this.properties.put("sslEndpointAlgorithm", sslEndpointAlgorithm);
            return (T) this;
        }
        /**
         * The algorithm used by key manager factory for SSL connections.
         * Default value is the key manager factory algorithm configured for the
         * Java Virtual Machine. The option is a java.lang.String type.
         */
        public T sslKeymanagerAlgorithm(String sslKeymanagerAlgorithm) {
            this.properties.put("sslKeymanagerAlgorithm", sslKeymanagerAlgorithm);
            return (T) this;
        }
        /**
         * The file format of the key store file. This is optional for client.
         * Default value is JKS. The option is a java.lang.String type.
         */
        public T sslKeystoreType(String sslKeystoreType) {
            this.properties.put("sslKeystoreType", sslKeystoreType);
            return (T) this;
        }
        /**
         * The SSL protocol used to generate the SSLContext. Default setting is
         * TLS, which is fine for most cases. Allowed values in recent JVMs are
         * TLS, TLSv1.1 and TLSv1.2. SSL, SSLv2 and SSLv3 may be supported in
         * older JVMs, but their usage is discouraged due to known security
         * vulnerabilities. The option is a java.lang.String type.
         */
        public T sslProtocol(String sslProtocol) {
            this.properties.put("sslProtocol", sslProtocol);
            return (T) this;
        }
        /**
         * The name of the security provider used for SSL connections. Default
         * value is the default security provider of the JVM. The option is a
         * java.lang.String type.
         */
        public T sslProvider(String sslProvider) {
            this.properties.put("sslProvider", sslProvider);
            return (T) this;
        }
        /**
         * The algorithm used by trust manager factory for SSL connections.
         * Default value is the trust manager factory algorithm configured for
         * the Java Virtual Machine. The option is a java.lang.String type.
         */
        public T sslTrustmanagerAlgorithm(String sslTrustmanagerAlgorithm) {
            this.properties.put("sslTrustmanagerAlgorithm", sslTrustmanagerAlgorithm);
            return (T) this;
        }
        /**
         * The file format of the trust store file. Default value is JKS. The
         * option is a java.lang.String type.
         */
        public T sslTruststoreType(String sslTruststoreType) {
            this.properties.put("sslTruststoreType", sslTruststoreType);
            return (T) this;
        }
        /**
         * URL of the Confluent Platform schema registry servers to use. The
         * format is host1:port1,host2:port2. This is known as
         * schema.registry.url in the Confluent Platform documentation. This
         * option is only available in the Confluent Platform (not standard
         * Apache Kafka). The option is a java.lang.String type.
         */
        public T schemaRegistryURL(String schemaRegistryURL) {
            this.properties.put("schemaRegistryURL", schemaRegistryURL);
            return (T) this;
        }
    }

    public static class KafkaConsumerBuilder
            extends
                KafkaCommonBuilder<KafkaConsumerBuilder>
            implements
                EndpointConsumerBuilder {
        public KafkaConsumerBuilder(String path) {
            super(path);
        }
        /**
         * Whether to allow doing manual commits via KafkaManualCommit. If this
         * option is enabled then an instance of KafkaManualCommit is stored on
         * the Exchange message header, which allows end users to access this
         * API and perform manual offset commits via the Kafka consumer. The
         * option is a boolean type.
         */
        public KafkaConsumerBuilder allowManualCommit(boolean allowManualCommit) {
            this.properties.put("allowManualCommit", allowManualCommit);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * If true, periodically commit to ZooKeeper the offset of messages
         * already fetched by the consumer. This committed offset will be used
         * when the process fails as the position from which the new consumer
         * will begin. The option is a java.lang.Boolean type.
         */
        public KafkaConsumerBuilder autoCommitEnable(Boolean autoCommitEnable) {
            this.properties.put("autoCommitEnable", autoCommitEnable);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The frequency in ms that the consumer offsets are committed to
         * zookeeper. The option is a java.lang.Integer type.
         */
        public KafkaConsumerBuilder autoCommitIntervalMs(
                Integer autoCommitIntervalMs) {
            this.properties.put("autoCommitIntervalMs", autoCommitIntervalMs);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * Whether to perform an explicit auto commit when the consumer stops to
         * ensure the broker has a commit from the last consumed message. This
         * requires the option autoCommitEnable is turned on. The possible
         * values are: sync, async, or none. And sync is the default value. The
         * option is a java.lang.String type.
         */
        public KafkaConsumerBuilder autoCommitOnStop(String autoCommitOnStop) {
            this.properties.put("autoCommitOnStop", autoCommitOnStop);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * What to do when there is no initial offset in ZooKeeper or if an
         * offset is out of range: earliest : automatically reset the offset to
         * the earliest offset latest : automatically reset the offset to the
         * latest offset fail: throw exception to the consumer. The option is a
         * java.lang.String type.
         */
        public KafkaConsumerBuilder autoOffsetReset(String autoOffsetReset) {
            this.properties.put("autoOffsetReset", autoOffsetReset);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * This options controls what happens when a consumer is processing an
         * exchange and it fails. If the option is false then the consumer
         * continues to the next message and processes it. If the option is true
         * then the consumer breaks out, and will seek back to offset of the
         * message that caused a failure, and then re-attempt to process this
         * message. However this can lead to endless processing of the same
         * message if its bound to fail every time, eg a poison message.
         * Therefore its recommended to deal with that for example by using
         * Camel's error handler. The option is a boolean type.
         */
        public KafkaConsumerBuilder breakOnFirstError(boolean breakOnFirstError) {
            this.properties.put("breakOnFirstError", breakOnFirstError);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * Allows for bridging the consumer to the Camel routing Error Handler,
         * which mean any exceptions occurred while the consumer is trying to
         * pickup incoming messages, or the likes, will now be processed as a
         * message and handled by the routing Error Handler. By default the
         * consumer will use the org.apache.camel.spi.ExceptionHandler to deal
         * with exceptions, that will be logged at WARN or ERROR level and
         * ignored. The option is a boolean type.
         */
        public KafkaConsumerBuilder bridgeErrorHandler(
                boolean bridgeErrorHandler) {
            this.properties.put("bridgeErrorHandler", bridgeErrorHandler);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * Automatically check the CRC32 of the records consumed. This ensures
         * no on-the-wire or on-disk corruption to the messages occurred. This
         * check adds some overhead, so it may be disabled in cases seeking
         * extreme performance. The option is a java.lang.Boolean type.
         */
        public KafkaConsumerBuilder checkCrcs(Boolean checkCrcs) {
            this.properties.put("checkCrcs", checkCrcs);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The configuration controls the maximum amount of time the client will
         * wait for the response of a request. If the response is not received
         * before the timeout elapses the client will resend the request if
         * necessary or fail the request if retries are exhausted. The option is
         * a java.lang.Integer type.
         */
        public KafkaConsumerBuilder consumerRequestTimeoutMs(
                Integer consumerRequestTimeoutMs) {
            this.properties.put("consumerRequestTimeoutMs", consumerRequestTimeoutMs);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The number of consumers that connect to kafka server. The option is a
         * int type.
         */
        public KafkaConsumerBuilder consumersCount(int consumersCount) {
            this.properties.put("consumersCount", consumersCount);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * Number of concurrent consumers on the consumer. The option is a int
         * type.
         */
        public KafkaConsumerBuilder consumerStreams(int consumerStreams) {
            this.properties.put("consumerStreams", consumerStreams);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The maximum amount of data the server should return for a fetch
         * request This is not an absolute maximum, if the first message in the
         * first non-empty partition of the fetch is larger than this value, the
         * message will still be returned to ensure that the consumer can make
         * progress. The maximum message size accepted by the broker is defined
         * via message.max.bytes (broker config) or max.message.bytes (topic
         * config). Note that the consumer performs multiple fetches in
         * parallel. The option is a java.lang.Integer type.
         */
        public KafkaConsumerBuilder fetchMaxBytes(Integer fetchMaxBytes) {
            this.properties.put("fetchMaxBytes", fetchMaxBytes);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The minimum amount of data the server should return for a fetch
         * request. If insufficient data is available the request will wait for
         * that much data to accumulate before answering the request. The option
         * is a java.lang.Integer type.
         */
        public KafkaConsumerBuilder fetchMinBytes(Integer fetchMinBytes) {
            this.properties.put("fetchMinBytes", fetchMinBytes);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The maximum amount of time the server will block before answering the
         * fetch request if there isn't sufficient data to immediately satisfy
         * fetch.min.bytes. The option is a java.lang.Integer type.
         */
        public KafkaConsumerBuilder fetchWaitMaxMs(Integer fetchWaitMaxMs) {
            this.properties.put("fetchWaitMaxMs", fetchWaitMaxMs);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * A string that uniquely identifies the group of consumer processes to
         * which this consumer belongs. By setting the same group id multiple
         * processes indicate that they are all part of the same consumer group.
         * This option is required for consumers. The option is a
         * java.lang.String type.
         */
        public KafkaConsumerBuilder groupId(String groupId) {
            this.properties.put("groupId", groupId);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The expected time between heartbeats to the consumer coordinator when
         * using Kafka's group management facilities. Heartbeats are used to
         * ensure that the consumer's session stays active and to facilitate
         * rebalancing when new consumers join or leave the group. The value
         * must be set lower than session.timeout.ms, but typically should be
         * set no higher than 1/3 of that value. It can be adjusted even lower
         * to control the expected time for normal rebalances. The option is a
         * java.lang.Integer type.
         */
        public KafkaConsumerBuilder heartbeatIntervalMs(
                Integer heartbeatIntervalMs) {
            this.properties.put("heartbeatIntervalMs", heartbeatIntervalMs);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * Sets custom KafkaHeaderDeserializer for deserialization kafka headers
         * values to camel headers values. The option is a
         * org.apache.camel.component.kafka.serde.KafkaHeaderDeserializer type.
         */
        public KafkaConsumerBuilder kafkaHeaderDeserializer(
                Object kafkaHeaderDeserializer) {
            this.properties.put("kafkaHeaderDeserializer", kafkaHeaderDeserializer);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * Deserializer class for key that implements the Deserializer
         * interface. The option is a java.lang.String type.
         */
        public KafkaConsumerBuilder keyDeserializer(String keyDeserializer) {
            this.properties.put("keyDeserializer", keyDeserializer);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The maximum amount of data per-partition the server will return. The
         * maximum total memory used for a request will be #partitions
         * max.partition.fetch.bytes. This size must be at least as large as the
         * maximum message size the server allows or else it is possible for the
         * producer to send messages larger than the consumer can fetch. If that
         * happens, the consumer can get stuck trying to fetch a large message
         * on a certain partition. The option is a java.lang.Integer type.
         */
        public KafkaConsumerBuilder maxPartitionFetchBytes(
                Integer maxPartitionFetchBytes) {
            this.properties.put("maxPartitionFetchBytes", maxPartitionFetchBytes);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The maximum delay between invocations of poll() when using consumer
         * group management. This places an upper bound on the amount of time
         * that the consumer can be idle before fetching more records. If poll()
         * is not called before expiration of this timeout, then the consumer is
         * considered failed and the group will rebalance in order to reassign
         * the partitions to another member. The option is a java.lang.Long
         * type.
         */
        public KafkaConsumerBuilder maxPollIntervalMs(Long maxPollIntervalMs) {
            this.properties.put("maxPollIntervalMs", maxPollIntervalMs);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The maximum number of records returned in a single call to poll().
         * The option is a java.lang.Integer type.
         */
        public KafkaConsumerBuilder maxPollRecords(Integer maxPollRecords) {
            this.properties.put("maxPollRecords", maxPollRecords);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The offset repository to use in order to locally store the offset of
         * each partition of the topic. Defining one will disable the
         * autocommit. The option is a
         * org.apache.camel.spi.StateRepository<java.lang.String,java.lang.String> type.
         */
        public KafkaConsumerBuilder offsetRepository(
                StateRepository<String, String> offsetRepository) {
            this.properties.put("offsetRepository", offsetRepository);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The class name of the partition assignment strategy that the client
         * will use to distribute partition ownership amongst consumer instances
         * when group management is used. The option is a java.lang.String type.
         */
        public KafkaConsumerBuilder partitionAssignor(String partitionAssignor) {
            this.properties.put("partitionAssignor", partitionAssignor);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The timeout used when polling the KafkaConsumer. The option is a
         * java.lang.Long type.
         */
        public KafkaConsumerBuilder pollTimeoutMs(Long pollTimeoutMs) {
            this.properties.put("pollTimeoutMs", pollTimeoutMs);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * Set if KafkaConsumer will read from beginning or end on startup:
         * beginning : read from beginning end : read from end This is replacing
         * the earlier property seekToBeginning. The option is a
         * java.lang.String type.
         */
        public KafkaConsumerBuilder seekTo(String seekTo) {
            this.properties.put("seekTo", seekTo);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * The timeout used to detect failures when using Kafka's group
         * management facilities. The option is a java.lang.Integer type.
         */
        public KafkaConsumerBuilder sessionTimeoutMs(Integer sessionTimeoutMs) {
            this.properties.put("sessionTimeoutMs", sessionTimeoutMs);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * This enables the use of a specific Avro reader for use with the
         * Confluent Platform schema registry and the
         * io.confluent.kafka.serializers.KafkaAvroDeserializer. This option is
         * only available in the Confluent Platform (not standard Apache Kafka).
         * The option is a boolean type.
         */
        public KafkaConsumerBuilder specificAvroReader(
                boolean specificAvroReader) {
            this.properties.put("specificAvroReader", specificAvroReader);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * Whether the topic is a pattern (regular expression). This can be used
         * to subscribe to dynamic number of topics matching the pattern. The
         * option is a boolean type.
         */
        public KafkaConsumerBuilder topicIsPattern(boolean topicIsPattern) {
            this.properties.put("topicIsPattern", topicIsPattern);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * Deserializer class for value that implements the Deserializer
         * interface. The option is a java.lang.String type.
         */
        public KafkaConsumerBuilder valueDeserializer(String valueDeserializer) {
            this.properties.put("valueDeserializer", valueDeserializer);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * To let the consumer use a custom ExceptionHandler. Notice if the
         * option bridgeErrorHandler is enabled then this option is not in use.
         * By default the consumer will deal with exceptions, that will be
         * logged at WARN or ERROR level and ignored. The option is a
         * org.apache.camel.spi.ExceptionHandler type.
         */
        public KafkaConsumerBuilder exceptionHandler(
                ExceptionHandler exceptionHandler) {
            this.properties.put("exceptionHandler", exceptionHandler);
            return (KafkaConsumerBuilder) this;
        }
        /**
         * Sets the exchange pattern when the consumer creates an exchange. The
         * option is a org.apache.camel.ExchangePattern type.
         */
        public KafkaConsumerBuilder exchangePattern(
                ExchangePattern exchangePattern) {
            this.properties.put("exchangePattern", exchangePattern);
            return (KafkaConsumerBuilder) this;
        }
    }

    public static class KafkaProducerBuilder
            extends
                KafkaCommonBuilder<KafkaProducerBuilder>
            implements
                EndpointProducerBuilder {
        public KafkaProducerBuilder(String path) {
            super(path);
        }
        /**
         * If the option is true, then KafkaProducer will ignore the
         * KafkaConstants.TOPIC header setting of the inbound message. The
         * option is a boolean type.
         */
        public KafkaProducerBuilder bridgeEndpoint(boolean bridgeEndpoint) {
            this.properties.put("bridgeEndpoint", bridgeEndpoint);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The total bytes of memory the producer can use to buffer records
         * waiting to be sent to the server. If records are sent faster than
         * they can be delivered to the server the producer will either block or
         * throw an exception based on the preference specified by
         * block.on.buffer.full.This setting should correspond roughly to the
         * total memory the producer will use, but is not a hard bound since not
         * all memory the producer uses is used for buffering. Some additional
         * memory will be used for compression (if compression is enabled) as
         * well as for maintaining in-flight requests. The option is a
         * java.lang.Integer type.
         */
        public KafkaProducerBuilder bufferMemorySize(Integer bufferMemorySize) {
            this.properties.put("bufferMemorySize", bufferMemorySize);
            return (KafkaProducerBuilder) this;
        }
        /**
         * If the option is true, then KafkaProducer will detect if the message
         * is attempted to be sent back to the same topic it may come from, if
         * the message was original from a kafka consumer. If the
         * KafkaConstants.TOPIC header is the same as the original kafka
         * consumer topic, then the header setting is ignored, and the topic of
         * the producer endpoint is used. In other words this avoids sending the
         * same message back to where it came from. This option is not in use if
         * the option bridgeEndpoint is set to true. The option is a boolean
         * type.
         */
        public KafkaProducerBuilder circularTopicDetection(
                boolean circularTopicDetection) {
            this.properties.put("circularTopicDetection", circularTopicDetection);
            return (KafkaProducerBuilder) this;
        }
        /**
         * This parameter allows you to specify the compression codec for all
         * data generated by this producer. Valid values are none, gzip and
         * snappy. The option is a java.lang.String type.
         */
        public KafkaProducerBuilder compressionCodec(String compressionCodec) {
            this.properties.put("compressionCodec", compressionCodec);
            return (KafkaProducerBuilder) this;
        }
        /**
         * Close idle connections after the number of milliseconds specified by
         * this config. The option is a java.lang.Integer type.
         */
        public KafkaProducerBuilder connectionMaxIdleMs(
                Integer connectionMaxIdleMs) {
            this.properties.put("connectionMaxIdleMs", connectionMaxIdleMs);
            return (KafkaProducerBuilder) this;
        }
        /**
         * If set to 'true' the producer will ensure that exactly one copy of
         * each message is written in the stream. If 'false', producer retries
         * may write duplicates of the retried message in the stream. If set to
         * true this option will require max.in.flight.requests.per.connection
         * to be set to 1 and retries cannot be zero and additionally acks must
         * be set to 'all'. The option is a boolean type.
         */
        public KafkaProducerBuilder enableIdempotence(boolean enableIdempotence) {
            this.properties.put("enableIdempotence", enableIdempotence);
            return (KafkaProducerBuilder) this;
        }
        /**
         * Sets custom KafkaHeaderDeserializer for serialization camel headers
         * values to kafka headers values. The option is a
         * org.apache.camel.component.kafka.serde.KafkaHeaderSerializer type.
         */
        public KafkaProducerBuilder kafkaHeaderSerializer(
                Object kafkaHeaderSerializer) {
            this.properties.put("kafkaHeaderSerializer", kafkaHeaderSerializer);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The record key (or null if no key is specified). If this option has
         * been configured then it take precedence over header
         * KafkaConstants#KEY. The option is a java.lang.String type.
         */
        public KafkaProducerBuilder key(String key) {
            this.properties.put("key", key);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The serializer class for keys (defaults to the same as for messages
         * if nothing is given). The option is a java.lang.String type.
         */
        public KafkaProducerBuilder keySerializerClass(String keySerializerClass) {
            this.properties.put("keySerializerClass", keySerializerClass);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The producer groups together any records that arrive in between
         * request transmissions into a single batched request. Normally this
         * occurs only under load when records arrive faster than they can be
         * sent out. However in some circumstances the client may want to reduce
         * the number of requests even under moderate load. This setting
         * accomplishes this by adding a small amount of artificial delaythat
         * is, rather than immediately sending out a record the producer will
         * wait for up to the given delay to allow other records to be sent so
         * that the sends can be batched together. This can be thought of as
         * analogous to Nagle's algorithm in TCP. This setting gives the upper
         * bound on the delay for batching: once we get batch.size worth of
         * records for a partition it will be sent immediately regardless of
         * this setting, however if we have fewer than this many bytes
         * accumulated for this partition we will 'linger' for the specified
         * time waiting for more records to show up. This setting defaults to 0
         * (i.e. no delay). Setting linger.ms=5, for example, would have the
         * effect of reducing the number of requests sent but would add up to
         * 5ms of latency to records sent in the absense of load. The option is
         * a java.lang.Integer type.
         */
        public KafkaProducerBuilder lingerMs(Integer lingerMs) {
            this.properties.put("lingerMs", lingerMs);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The configuration controls how long sending to kafka will block.
         * These methods can be blocked for multiple reasons. For e.g: buffer
         * full, metadata unavailable.This configuration imposes maximum limit
         * on the total time spent in fetching metadata, serialization of key
         * and value, partitioning and allocation of buffer memory when doing a
         * send(). In case of partitionsFor(), this configuration imposes a
         * maximum time threshold on waiting for metadata. The option is a
         * java.lang.Integer type.
         */
        public KafkaProducerBuilder maxBlockMs(Integer maxBlockMs) {
            this.properties.put("maxBlockMs", maxBlockMs);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The maximum number of unacknowledged requests the client will send on
         * a single connection before blocking. Note that if this setting is set
         * to be greater than 1 and there are failed sends, there is a risk of
         * message re-ordering due to retries (i.e., if retries are enabled).
         * The option is a java.lang.Integer type.
         */
        public KafkaProducerBuilder maxInFlightRequest(
                Integer maxInFlightRequest) {
            this.properties.put("maxInFlightRequest", maxInFlightRequest);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The maximum size of a request. This is also effectively a cap on the
         * maximum record size. Note that the server has its own cap on record
         * size which may be different from this. This setting will limit the
         * number of record batches the producer will send in a single request
         * to avoid sending huge requests. The option is a java.lang.Integer
         * type.
         */
        public KafkaProducerBuilder maxRequestSize(Integer maxRequestSize) {
            this.properties.put("maxRequestSize", maxRequestSize);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The period of time in milliseconds after which we force a refresh of
         * metadata even if we haven't seen any partition leadership changes to
         * proactively discover any new brokers or partitions. The option is a
         * java.lang.Integer type.
         */
        public KafkaProducerBuilder metadataMaxAgeMs(Integer metadataMaxAgeMs) {
            this.properties.put("metadataMaxAgeMs", metadataMaxAgeMs);
            return (KafkaProducerBuilder) this;
        }
        /**
         * A list of classes to use as metrics reporters. Implementing the
         * MetricReporter interface allows plugging in classes that will be
         * notified of new metric creation. The JmxReporter is always included
         * to register JMX statistics. The option is a java.lang.String type.
         */
        public KafkaProducerBuilder metricReporters(String metricReporters) {
            this.properties.put("metricReporters", metricReporters);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The number of samples maintained to compute metrics. The option is a
         * java.lang.Integer type.
         */
        public KafkaProducerBuilder metricsSampleWindowMs(
                Integer metricsSampleWindowMs) {
            this.properties.put("metricsSampleWindowMs", metricsSampleWindowMs);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The number of samples maintained to compute metrics. The option is a
         * java.lang.Integer type.
         */
        public KafkaProducerBuilder noOfMetricsSample(Integer noOfMetricsSample) {
            this.properties.put("noOfMetricsSample", noOfMetricsSample);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The partitioner class for partitioning messages amongst sub-topics.
         * The default partitioner is based on the hash of the key. The option
         * is a java.lang.String type.
         */
        public KafkaProducerBuilder partitioner(String partitioner) {
            this.properties.put("partitioner", partitioner);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The partition to which the record will be sent (or null if no
         * partition was specified). If this option has been configured then it
         * take precedence over header KafkaConstants#PARTITION_KEY. The option
         * is a java.lang.Integer type.
         */
        public KafkaProducerBuilder partitionKey(Integer partitionKey) {
            this.properties.put("partitionKey", partitionKey);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The producer will attempt to batch records together into fewer
         * requests whenever multiple records are being sent to the same
         * partition. This helps performance on both the client and the server.
         * This configuration controls the default batch size in bytes. No
         * attempt will be made to batch records larger than this size.Requests
         * sent to brokers will contain multiple batches, one for each partition
         * with data available to be sent.A small batch size will make batching
         * less common and may reduce throughput (a batch size of zero will
         * disable batching entirely). A very large batch size may use memory a
         * bit more wastefully as we will always allocate a buffer of the
         * specified batch size in anticipation of additional records. The
         * option is a java.lang.Integer type.
         */
        public KafkaProducerBuilder producerBatchSize(Integer producerBatchSize) {
            this.properties.put("producerBatchSize", producerBatchSize);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The maximum number of unsent messages that can be queued up the
         * producer when using async mode before either the producer must be
         * blocked or data must be dropped. The option is a java.lang.Integer
         * type.
         */
        public KafkaProducerBuilder queueBufferingMaxMessages(
                Integer queueBufferingMaxMessages) {
            this.properties.put("queueBufferingMaxMessages", queueBufferingMaxMessages);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The size of the TCP receive buffer (SO_RCVBUF) to use when reading
         * data. The option is a java.lang.Integer type.
         */
        public KafkaProducerBuilder receiveBufferBytes(
                Integer receiveBufferBytes) {
            this.properties.put("receiveBufferBytes", receiveBufferBytes);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The amount of time to wait before attempting to reconnect to a given
         * host. This avoids repeatedly connecting to a host in a tight loop.
         * This backoff applies to all requests sent by the consumer to the
         * broker. The option is a java.lang.Integer type.
         */
        public KafkaProducerBuilder reconnectBackoffMs(
                Integer reconnectBackoffMs) {
            this.properties.put("reconnectBackoffMs", reconnectBackoffMs);
            return (KafkaProducerBuilder) this;
        }
        /**
         * Whether the producer should store the RecordMetadata results from
         * sending to Kafka. The results are stored in a List containing the
         * RecordMetadata metadata's. The list is stored on a header with the
         * key KafkaConstants#KAFKA_RECORDMETA. The option is a boolean type.
         */
        public KafkaProducerBuilder recordMetadata(boolean recordMetadata) {
            this.properties.put("recordMetadata", recordMetadata);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The number of acknowledgments the producer requires the leader to
         * have received before considering a request complete. This controls
         * the durability of records that are sent. The following settings are
         * common: acks=0 If set to zero then the producer will not wait for any
         * acknowledgment from the server at all. The record will be immediately
         * added to the socket buffer and considered sent. No guarantee can be
         * made that the server has received the record in this case, and the
         * retries configuration will not take effect (as the client won't
         * generally know of any failures). The offset given back for each
         * record will always be set to -1. acks=1 This will mean the leader
         * will write the record to its local log but will respond without
         * awaiting full acknowledgement from all followers. In this case should
         * the leader fail immediately after acknowledging the record but before
         * the followers have replicated it then the record will be lost.
         * acks=all This means the leader will wait for the full set of in-sync
         * replicas to acknowledge the record. This guarantees that the record
         * will not be lost as long as at least one in-sync replica remains
         * alive. This is the strongest available guarantee. The option is a
         * java.lang.String type.
         */
        public KafkaProducerBuilder requestRequiredAcks(
                String requestRequiredAcks) {
            this.properties.put("requestRequiredAcks", requestRequiredAcks);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The amount of time the broker will wait trying to meet the
         * request.required.acks requirement before sending back an error to the
         * client. The option is a java.lang.Integer type.
         */
        public KafkaProducerBuilder requestTimeoutMs(Integer requestTimeoutMs) {
            this.properties.put("requestTimeoutMs", requestTimeoutMs);
            return (KafkaProducerBuilder) this;
        }
        /**
         * Setting a value greater than zero will cause the client to resend any
         * record whose send fails with a potentially transient error. Note that
         * this retry is no different than if the client resent the record upon
         * receiving the error. Allowing retries will potentially change the
         * ordering of records because if two records are sent to a single
         * partition, and the first fails and is retried but the second
         * succeeds, then the second record may appear first. The option is a
         * java.lang.Integer type.
         */
        public KafkaProducerBuilder retries(Integer retries) {
            this.properties.put("retries", retries);
            return (KafkaProducerBuilder) this;
        }
        /**
         * Before each retry, the producer refreshes the metadata of relevant
         * topics to see if a new leader has been elected. Since leader election
         * takes a bit of time, this property specifies the amount of time that
         * the producer waits before refreshing the metadata. The option is a
         * java.lang.Integer type.
         */
        public KafkaProducerBuilder retryBackoffMs(Integer retryBackoffMs) {
            this.properties.put("retryBackoffMs", retryBackoffMs);
            return (KafkaProducerBuilder) this;
        }
        /**
         * Socket write buffer size. The option is a java.lang.Integer type.
         */
        public KafkaProducerBuilder sendBufferBytes(Integer sendBufferBytes) {
            this.properties.put("sendBufferBytes", sendBufferBytes);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The serializer class for messages. The option is a java.lang.String
         * type.
         */
        public KafkaProducerBuilder serializerClass(String serializerClass) {
            this.properties.put("serializerClass", serializerClass);
            return (KafkaProducerBuilder) this;
        }
        /**
         * To use a custom worker pool for continue routing Exchange after kafka
         * server has acknowledge the message that was sent to it from
         * KafkaProducer using asynchronous non-blocking processing. The option
         * is a java.util.concurrent.ExecutorService type.
         */
        public KafkaProducerBuilder workerPool(ExecutorService workerPool) {
            this.properties.put("workerPool", workerPool);
            return (KafkaProducerBuilder) this;
        }
        /**
         * Number of core threads for the worker pool for continue routing
         * Exchange after kafka server has acknowledge the message that was sent
         * to it from KafkaProducer using asynchronous non-blocking processing.
         * The option is a java.lang.Integer type.
         */
        public KafkaProducerBuilder workerPoolCoreSize(
                Integer workerPoolCoreSize) {
            this.properties.put("workerPoolCoreSize", workerPoolCoreSize);
            return (KafkaProducerBuilder) this;
        }
        /**
         * Maximum number of threads for the worker pool for continue routing
         * Exchange after kafka server has acknowledge the message that was sent
         * to it from KafkaProducer using asynchronous non-blocking processing.
         * The option is a java.lang.Integer type.
         */
        public KafkaProducerBuilder workerPoolMaxSize(Integer workerPoolMaxSize) {
            this.properties.put("workerPoolMaxSize", workerPoolMaxSize);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The password of the private key in the key store file. This is
         * optional for client. The option is a java.lang.String type.
         */
        public KafkaProducerBuilder sslKeyPassword(String sslKeyPassword) {
            this.properties.put("sslKeyPassword", sslKeyPassword);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The location of the key store file. This is optional for client and
         * can be used for two-way authentication for client. The option is a
         * java.lang.String type.
         */
        public KafkaProducerBuilder sslKeystoreLocation(
                String sslKeystoreLocation) {
            this.properties.put("sslKeystoreLocation", sslKeystoreLocation);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The store password for the key store file.This is optional for client
         * and only needed if ssl.keystore.location is configured. The option is
         * a java.lang.String type.
         */
        public KafkaProducerBuilder sslKeystorePassword(
                String sslKeystorePassword) {
            this.properties.put("sslKeystorePassword", sslKeystorePassword);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The location of the trust store file. The option is a
         * java.lang.String type.
         */
        public KafkaProducerBuilder sslTruststoreLocation(
                String sslTruststoreLocation) {
            this.properties.put("sslTruststoreLocation", sslTruststoreLocation);
            return (KafkaProducerBuilder) this;
        }
        /**
         * The password for the trust store file. The option is a
         * java.lang.String type.
         */
        public KafkaProducerBuilder sslTruststorePassword(
                String sslTruststorePassword) {
            this.properties.put("sslTruststorePassword", sslTruststorePassword);
            return (KafkaProducerBuilder) this;
        }
    }
    public default KafkaConsumerBuilder fromKafka(String path) {
        return new KafkaConsumerBuilder(path);
    }
    public default KafkaProducerBuilder toKafka(String path) {
        return new KafkaProducerBuilder(path);
    }
}